{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASD\\anaconda3\\envs\\pytorchenv\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train_loss: 0.044756897329504976\n",
      "test_loss: 0.3842197222014268\n",
      "Epoch 2, train_loss: 0.03492586689642052\n",
      "test_loss: 0.36963170228732956\n",
      "Epoch 3, train_loss: 0.03325567696413963\n",
      "test_loss: 0.367331893907653\n",
      "Epoch 4, train_loss: 0.03133594176543304\n",
      "test_loss: 0.369563566106889\n",
      "Epoch 5, train_loss: 0.0289524326624434\n",
      "test_loss: 0.37096358297599685\n",
      "Epoch 6, train_loss: 0.027393649947019965\n",
      "test_loss: 0.3730640320314301\n",
      "Epoch 7, train_loss: 0.025156307186371362\n",
      "test_loss: 0.38547680821890634\n",
      "Epoch 8, train_loss: 0.021222296712640063\n",
      "test_loss: 0.46947262845868765\n",
      "Epoch 9, train_loss: 0.02108464575086544\n",
      "test_loss: 0.44884899545448004\n",
      "Epoch 10, train_loss: 0.016937596626044098\n",
      "test_loss: 0.49029716747301993\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "import torch.utils.data as data_utils\n",
    "#from pytorch_pretrained_bert import BertTokenizer, BertModel, BertConfig\n",
    "# def pad_sequence(sequence,max_length):\n",
    "#     if len(sequence)<max_length:\n",
    "#         sequence += (max_length-len(sequence))*[0]\n",
    "#     return sequence\n",
    "\n",
    "def embeddings_from_dataset(X, tokenizer, bert_model):\n",
    "\n",
    "    X = [tokenizer.tokenize('[CLS] ' + sent + ' [SEP]') for sent in X] # Appending [CLS] and [SEP] tokens - this probably can be done in a cleaner way\n",
    "    #X_test = [tokenizer.tokenize('[CLS] ' + sent + ' [SEP]') for sent in X_test] # Appending [CLS] and [SEP] tokens - this probably can be done in a cleaner way\n",
    "    X = [text[:512] if len(text)>512 else text for text in X]\n",
    "    #X_test = [text[:512] if len(text)>512 else text for text in X_test]\n",
    "    X_tokens = [tokenizer.convert_tokens_to_ids(sent) for sent in X]\n",
    "    #X_tokens = [pad_sequence(sequence) if len(sequence)<512 else sequence for sequence in X_tokens]\n",
    "\n",
    "    #X_test_tokens = [tokenizer.convert_tokens_to_ids(sent) for sent in X_test]\n",
    "\n",
    "    train_embeddings = []\n",
    "    #test_embeddings = []\n",
    "\n",
    "    #results = torch.zeros((len(X_train_tokens), bert_model.config.hidden_size)).long()\n",
    "    with torch.no_grad():\n",
    "        for stidx in range(len(X_tokens)):\n",
    "            tokens = X_tokens[stidx]\n",
    "            tokens_t = torch.LongTensor(tokens)#.to(device)\n",
    "            segment_t = torch.LongTensor([1] * len(tokens))#.to(device)\n",
    "            outputs = bert_model(tokens_t.unsqueeze(0),segment_t.unsqueeze(0))\n",
    "            embeddings = outputs[0][0][0] #This only takes CLS embedding\n",
    "            train_embeddings.append(embeddings.cpu())\n",
    "            #results[stidx] = embeddings.cpu()\n",
    "        \n",
    "        # for stidx in range(len(X_test)):\n",
    "        #     tokens = X_test_tokens[stidx]\n",
    "        #     tokens_t = torch.LongTensor(tokens)#.to(device)\n",
    "        #     segment_t = torch.LongTensor([1] * len(tokens))#.to(device)\n",
    "        #     outputs = bert_model(tokens_t.unsqueeze(0),segment_t.unsqueeze(0))\n",
    "        #     embeddings = outputs[0][0][0] #This only takes CLS embedding\n",
    "        #     test_embeddings.append(embeddings.cpu())\n",
    "    return torch.stack(train_embeddings)\n",
    "\n",
    "def train_model(model,optimizer,criterion,loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        x = data[0]\n",
    "        label = data[1]\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss/len(loader.dataset)    \n",
    "\n",
    "def test_model(model,criterion,loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        x = data[0]\n",
    "        label = data[1]\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred,label)\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss/len(loader.dataset)\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, input_size = 768):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size,100)\n",
    "        self.fc2 = nn.Linear(100,50)\n",
    "        self.fc3 = nn.Linear(50,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "corpus_reddit = pd.read_csv('reddit_corpus_agree.csv')\n",
    "corpus_reddit['label'] = 0\n",
    "corpus_reddit['label'].loc[(corpus_reddit['cls']=='Risk')] = 1 \n",
    "del corpus_reddit['cls']\n",
    "\n",
    "batch_size = 1\n",
    "texts, labels =  corpus_reddit['text'], corpus_reddit['label']\n",
    "#texts = [\" \".join(text.split()[:512]) if len(text.split())>512 else text for text in texts]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.1, stratify=corpus_reddit['label'])\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config = BertConfig.from_pretrained(model_name, output_hidden_states=True)    \n",
    "bert_model = BertModel.from_pretrained(model_name, config=config)\n",
    "bert_model = bert_model#.to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "train_embeddings = embeddings_from_dataset(X_train, tokenizer, bert_model)\n",
    "test_embeddings = embeddings_from_dataset(X_test, tokenizer, bert_model)\n",
    "\n",
    "train_dataset = data_utils.TensorDataset(train_embeddings , torch.FloatTensor(y_train.values).view(-1,1))\n",
    "test_dataset = data_utils.TensorDataset(test_embeddings , torch.FloatTensor(y_test.values).view(-1,1))\n",
    "\n",
    "train_loader = data_utils.DataLoader(train_dataset,batch_size=10,shuffle=True)\n",
    "test_loader = data_utils.DataLoader(test_dataset,batch_size=1)\n",
    "\n",
    "my_NN = BertClassifier()\n",
    "optimizer = torch.optim.Adam(my_NN.parameters(),lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    loss = train_model(my_NN,optimizer,criterion,train_loader)\n",
    "    print(f\"Epoch {epoch+1}, train_loss: {loss}\")\n",
    "    test_loss = test_model(my_NN,criterion,test_loader)\n",
    "    print(f\"test_loss: {test_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new(): argument 'size' must be tuple of ints, but found element of type Tensor at pos 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8372/269208405.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mBertClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeneme\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8372/3410191016.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_size)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m768\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorchenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, in_features, out_features, bias)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0min_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: new(): argument 'size' must be tuple of ints, but found element of type Tensor at pos 2"
     ]
    }
   ],
   "source": [
    "deneme = embeddings_from_dataset([X_train[0]],tokenizer,bert_model)\n",
    "with torch.no_grad():\n",
    "    BertClassifier(deneme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9921]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_NN(deneme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9769]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "deneme = embeddings_from_dataset([X_test.iloc[-6]],tokenizer,bert_model)\n",
    "with torch.no_grad():\n",
    "    print(my_NN(deneme))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44     1\n",
       "94     1\n",
       "56     1\n",
       "9      1\n",
       "75     1\n",
       "144    1\n",
       "170    1\n",
       "61     1\n",
       "31     1\n",
       "163    1\n",
       "58     1\n",
       "70     1\n",
       "111    0\n",
       "22     1\n",
       "27     0\n",
       "93     1\n",
       "26     1\n",
       "127    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120    1\n",
       "54     1\n",
       "48     1\n",
       "159    1\n",
       "135    1\n",
       "      ..\n",
       "145    0\n",
       "59     1\n",
       "32     1\n",
       "62     0\n",
       "24     1\n",
       "Name: label, Length: 153, dtype: int64"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My secret suicidal thoughts are getting worse. It\\'s getting harder to disobey. And I dont want to tell anyone\\r\\nI\\'m one of those super annoying people pleaser types. I\\'m constantly doing favors. Listening to other people\\'s problems. And forever smiling and trying to keep the positive vibes going.\\r\\n\\r\\nI\\'m worse than just all that though. Because im an airhead and I dont have good social skills. I\\'m always saying or doing something stupid. And it makes me more awkward. I act like it\\'s all good and I\\'m happy and it doesnt phase me. But that is so far from the truth. \\r\\n\\r\\nEvery single day I\\'m plagued by thoughts of how weak I am and how I\\'m annoying and how I\\'m a bitch and \"I need to die\" over and over and over. Which turns into \"I wanna kill myself\" over and over. It\\'s getting worse every day. I dont expect help. I just needed a place to vent.'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.values[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.values[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a442c826b45f4c3a7cead15d3b3e25b74e84b5906d9e45699e0abfc2d5a0e9a4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 ('pytorchenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
