# -*- coding: utf-8 -*-
"""NLP_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xsOgkDgFhUyx_E4bCpfi7G4pnuEdaKrd
"""

import numpy as np
import pandas as pd

import numpy as np
import pandas as pd


data = pd.read_csv("/content/Suicide_Detection.csv")
data.head(100)

pip  install pyforest

pip  install twint

pip  install nest_asyncio

# Import pyforest (PyForest imports over 40 libraries, such as Pandas and Numpy)
import pyforest
# Import Twint and Nest
import twint
import nest_asyncio
# NLP Packages
from nltk.corpus import stopwords
from textblob import TextBlob 
from textblob import Word
import string
from sklearn.feature_extraction.text import CountVectorizer
from collections import Counter

data_neg_samples0 = pd.read_csv("/content/Suicide_Detection.csv")
data_neg_samples0.head(20)

import nltk
nltk.download("all")

punctuation = '-’—“—”!"#$%&\'()*+,-./:;<=>-?@[\\]^_`{|}~'
for index, row in data_neg_samples0.iterrows():
  a = row['text'].split()
  for word in a:
    #str(TextBlob(word).correct) 
    word = word.lower()
    word = word.translate(str.maketrans('', '', punctuation))

# Python program to convert a list
# to string using join() function
    
# Function to convert  
def listToString(s): 
    
    # initialize an empty string
    str1 = " " 
    
    # return string  
    return (str1.join(s))
        
        
# Driver code    
s = ['Geeks', 'for', 'Geeks']
print(listToString(s))

punctuation = '-’—“—”!"#$%&\'()*+,-./:;<=>-?@[\\]^_`{|}~'
stop_words = ["it", "its", "itself", "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these", "those", "be", "been", "being", "had", "having", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before", "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again", "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than", "too", "very", "s", "t", "just", "don", "should", "now"]

for index, row in data_neg_samples0.iterrows():
  tokenizer = nltk.RegexpTokenizer(r"\w+")
  new_words = tokenizer.tokenize(row['text'])
  addition = []
  for i in new_words:
    i = i.lower()
    i = i.translate(str.maketrans('', '', punctuation))
    if (i not in stop_words):
      #new_words.remove(i)
      addition.append(i)
  #data_neg_samples0['text'][index] = listToString(new_words)
  data_neg_samples0['text'][index] = listToString(addition)
  #row['text'] = new_words

data_neg_samples0.head(50)

data_neg_samples0.to_csv("/content/pre_processed.csv")